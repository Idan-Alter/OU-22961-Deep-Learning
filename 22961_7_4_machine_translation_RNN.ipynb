{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMRV4iMcN2bJe3q3HRZEkwg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Idan-Alter/OU-22961-Deep-Learning/blob/main/22961_7_4_machine_translation_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-rkogg_jh6N"
      },
      "outputs": [],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import datasets as ds"
      ],
      "metadata": {
        "id": "0SsirZB5kviZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src = \"en\"\n",
        "tgt = \"fr\"\n",
        "dataset = ds.load_dataset(\"tatoeba\", lang1=src, lang2=tgt)\n",
        "dataset"
      ],
      "metadata": {
        "id": "2di34Icnkjpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][\"translation\"][10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_LDNUr2k8Pb",
        "outputId": "b88ae423-d18f-490f-cfa7-bc637bc2758b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'en': \"Today is June 18th and it is Muiriel's birthday!\",\n",
              " 'fr': \"Aujourd'hui nous sommes le 18 juin et c'est l'anniversaire de Muiriel !\"}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs_list = dataset[\"train\"][\"translation\"]\n",
        "total = len(pairs_list)\n",
        "src_sents_unfiltered = [x[src].split() for x in pairs_list]\n",
        "tgt_sents_unfiltered = [x[tgt].split() for x in pairs_list]"
      ],
      "metadata": {
        "id": "5y_-nclbr4N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_length = 5\n",
        "filter = lambda idx: len(src_sents_unfiltered[idx]) <= MAX_length and \\\n",
        "                     len(tgt_sents_unfiltered[idx]) <= MAX_length\n",
        "mask = map(filter, range(total))\n",
        "src_sents = [x for idx,x in enumerate(src_sents_unfiltered) if filter(idx)]\n",
        "tgt_sents = [x for idx,x in enumerate(tgt_sents_unfiltered) if filter(idx)]"
      ],
      "metadata": {
        "id": "BKEYrL_4up5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shuffle the data\n",
        "torch.manual_seed(0)\n",
        "shuffle_idxs = torch.randperm(len(src_sents))\n",
        "def shuffle(my_list):\n",
        "  extract_one   = lambda x: my_list[shuffle_idxs[x]]\n",
        "  shuffled_list = list(map(extract_one,range(len(my_list))))\n",
        "  return shuffled_list\n",
        "src_sents = shuffle(src_sents)\n",
        "tgt_sents = shuffle(tgt_sents)"
      ],
      "metadata": {
        "id": "YzellgFAvagi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(len(src_sents)):\n",
        "  src_sents[idx].append(\"<END>\")\n",
        "  tgt_sents[idx] = [\"<START>\"]+tgt_sents[idx]+[\"<END>\"]"
      ],
      "metadata": {
        "id": "e-faCAHN5tOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "src_vocab = build_vocab_from_iterator(src_sents, specials=[\"<UNK>\",\"<END>\"])\n",
        "src_vocab.set_default_index(0)\n",
        "print(len(src_vocab))\n",
        "tgt_vocab = build_vocab_from_iterator(tgt_sents, specials=[\"<UNK>\",\"<END>\",\"<START>\"])\n",
        "tgt_vocab.set_default_index(0)\n",
        "print(len(tgt_vocab))\n",
        "\n",
        "src_tokens = list(map(lambda x: torch.tensor(src_vocab(x)), src_sents))\n",
        "tgt_tokens = list(map(lambda x: torch.tensor(tgt_vocab(x)), tgt_sents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44rM3EQBsJz3",
        "outputId": "ab589714-88e5-46b7-94ed-6e5221e67ab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18698\n",
            "27836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "START_Token = torch.tensor(tgt_vocab([\"<START>\"])[0])\n",
        "END_Token   = torch.tensor(tgt_vocab([\"<END>\"])[0])\n",
        "print(START_Token, END_Token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28c99WGtNpoU",
        "outputId": "f8796f92-d42b-47d3-e8ea-e29a8e829747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2) tensor(1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tgt_vocab.get_itos()[0:15])\n",
        "print(src_vocab.get_itos()[0:15])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYjHQz5O00UP",
        "outputId": "c48eac95-75bc-4ada-c1d1-6396bf5b0ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<UNK>', '<END>', '<START>', '?', 'Je', 'est', '!', 'Il', 'pas', 'Tom', 'de', 'un', \"C'est\", 'le', 'a']\n",
            "['<UNK>', '<END>', 'I', 'is', 'a', 'you', 'the', 'to', 'Tom', 'He', 'was', \"I'm\", 'You', 'She', 'The']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Source:\", src_sents[0], src_tokens[0], sep=\"\\n\")\n",
        "print(\"Target:\", tgt_sents[0], tgt_tokens[0], sep=\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPXt2ZGM8POS",
        "outputId": "a947f6b0-7cda-4635-c974-3af31d4c8642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source:\n",
            "['I', 'think', 'that', 'helps.', '<END>']\n",
            "tensor([   2,  140,   43, 4795,    1])\n",
            "Target:\n",
            "['<START>', 'Je', 'pense', 'que', 'ça', 'aide.', '<END>']\n",
            "tensor([  2,   4, 184,  39,  75, 644,   1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim, RNNlayers):\n",
        "        super().__init__()\n",
        "        self.src_embedding  = nn.Embedding(len(src_vocab),\n",
        "                                           embed_dim)  \n",
        "        self.rnn_stack      = nn.LSTM(embed_dim,                   \n",
        "                                 hidden_dim,\n",
        "                                 RNNlayers)  \n",
        "    def forward(self, src_tokens):\n",
        "      all_embeddings         = self.src_embedding(src_tokens)\n",
        "      all_embeddings         = all_embeddings.unsqueeze(1)\n",
        "      hidden_state_history, _= self.rnn_stack(all_embeddings)\n",
        "      context                = hidden_state_history[-1,0,:]\n",
        "      return context"
      ],
      "metadata": {
        "id": "cuP2INO39sPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNNCell(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_state  = torch.zeros(hidden_dim)\n",
        "        self.RNNcell       = nn.RNNCell(embed_dim, hidden_dim)\n",
        "        self.output_linear = nn.Linear(in_features=hidden_dim,\n",
        "                                  out_features=len(tgt_vocab))\n",
        "        self.logsoftmax    = nn.LogSoftmax(dim=0)\n",
        "        \n",
        "    def forward(self, one_embedded_token):\n",
        "        new_state          = self.RNNcell(one_embedded_token,\n",
        "                                           self.hidden_state)\n",
        "        tgt_token_scores   = self.output_linear(new_state)\n",
        "        tgt_token_logprobs = self.logsoftmax(tgt_token_scores)\n",
        "        self.hidden_state = new_state \n",
        "        return tgt_token_logprobs"
      ],
      "metadata": {
        "id": "rBXl32g6DAaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingDecoder(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.tgt_embedding  = nn.Embedding(len(tgt_vocab),\n",
        "                                           embed_dim)  \n",
        "        self.RNNcell        = DecoderRNNCell(embed_dim,\n",
        "                                             hidden_dim)  \n",
        "    def forward(self, context, tgt_tokens):\n",
        "      self.RNNcell.hidden_state = context\n",
        "      translated_tokens = [START_Token]\n",
        "      sentence_loss = 0\n",
        "      for idx in range(len(tgt_tokens)-1):\n",
        "        ##Teacher forcing:\n",
        "        #previous_token  = tgt_tokens[idx] \n",
        "        previous_token  = translated_tokens[idx]   \n",
        "        embedded_token  = self.tgt_embedding(previous_token)\n",
        "        logprobs        = self.RNNcell(embedded_token)\n",
        "        predicted_token = logprobs.argmax()\n",
        "        translated_tokens.append(predicted_token.detach())\n",
        "        \n",
        "        correct_token   = tgt_tokens[idx+1]                 #\n",
        "        token_loss      = -logprobs[correct_token]          #\n",
        "        sentence_loss  += token_loss                        #\n",
        "        \n",
        "        if predicted_token == END_Token:\n",
        "          break\n",
        "      return translated_tokens, sentence_loss"
      ],
      "metadata": {
        "id": "YbEzW5vm-Tr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingTranslator(nn.Module):\n",
        "      def __init__(self, embed_dim, hidden_dim, encoder_layers):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(embed_dim, hidden_dim, encoder_layers)\n",
        "        self.decoder = TrainingDecoder(embed_dim, hidden_dim)\n",
        "      def forward(self, src_tokens, tgt_tokens):\n",
        "        context = self.encoder(src_tokens)\n",
        "        return self.decoder(context, tgt_tokens)"
      ],
      "metadata": {
        "id": "r-mWWFJ_o-dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Eval Mode"
      ],
      "metadata": {
        "id": "gkgB31-j84Aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(TrainingDecoder):\n",
        "    def __init__(self, embed_dim, hidden_dim):\n",
        "        super().__init__(embed_dim, hidden_dim)\n",
        "    def forward(self,context, tgt_tokens=None, max_tokens=10):\n",
        "      if self.training:\n",
        "        return super().forward(context, tgt_tokens)\n",
        "      else:\n",
        "        with torch.no_grad():\n",
        "          self.RNNcell.hidden_state = context\n",
        "          translated_tokens = [START_Token]\n",
        "          current_token = translated_tokens[0]\n",
        "          for _ in range(max_tokens):\n",
        "            embedded_token  = self.tgt_embedding(current_token)\n",
        "            logprobs        = self.RNNcell(embedded_token)\n",
        "            predicted_token = logprobs.argmax()\n",
        "            translated_tokens.append(predicted_token.detach())\n",
        "            if predicted_token == END_Token:\n",
        "              break\n",
        "            current_token   = predicted_token \n",
        "        return translated_tokens"
      ],
      "metadata": {
        "id": "LkaRcdDf2nkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(nn.Module):\n",
        "      def __init__(self, embed_dim, hidden_dim,encoder_layers):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(embed_dim, hidden_dim, encoder_layers)\n",
        "        self.decoder = Decoder(embed_dim, hidden_dim)\n",
        "      def forward(self,src_tokens, tgt_tokens=None):\n",
        "        context = self.encoder(src_tokens)\n",
        "        if self.training:\n",
        "          out=self.decoder(context, tgt_tokens)\n",
        "        else:\n",
        "          out=self.decoder(context)\n",
        "        return out"
      ],
      "metadata": {
        "id": "14oj-GfjegBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "Y8cKZw2iecgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iterate_one_pair(src_tokens, tgt_tokens):\n",
        "    model.train()  \n",
        "    optimizer.zero_grad()\n",
        "    output, loss = model(src_tokens, tgt_tokens)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.detach()"
      ],
      "metadata": {
        "id": "Yt76b77Worze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model     = Translator(50,50,2)\n",
        "optimizer = torch.optim.AdamW(model.parameters())"
      ],
      "metadata": {
        "id": "9S28peMPJMDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#overfit a small batch to check if learning _can_ occur\n",
        "num_samples, epochs = 10, 200\n",
        "for epoch in range(epochs):\n",
        "  batch_loss_agg = torch.tensor([0.])\n",
        "  for idx in range(num_samples):\n",
        "    batch_loss_agg += iterate_one_pair(src_tokens[idx], tgt_tokens[idx])\n",
        "  epoch_loss = batch_loss_agg / num_samples\n",
        "  if epoch % 20 == 0:\n",
        "    print(\"Epoch\", epoch, \" loss:\", epoch_loss.item())"
      ],
      "metadata": {
        "id": "EjEYevo7oe6j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fbd8a0f-ef0b-46fe-a969-2a3ddafbfb52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0  loss: 51.42719650268555\n",
            "Epoch 20  loss: 8.938413619995117\n",
            "Epoch 40  loss: 7.273235321044922\n",
            "Epoch 60  loss: 7.895596981048584\n",
            "Epoch 80  loss: 5.613731861114502\n",
            "Epoch 100  loss: 6.131002426147461\n",
            "Epoch 120  loss: 3.0154340267181396\n",
            "Epoch 140  loss: 1.5002210140228271\n",
            "Epoch 160  loss: 0.8781587481498718\n",
            "Epoch 180  loss: 0.5741285085678101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  for idx in range(num_samples):\n",
        "    a = model(src_tokens[idx])\n",
        "    predicted_itos = [tgt_vocab.get_itos()[x.item()] for x in a] \n",
        "    ground_truth   = [tgt_vocab.get_itos()[x.item()] for x in tgt_tokens[idx]] \n",
        "    print(predicted_itos, ground_truth)"
      ],
      "metadata": {
        "id": "11bVM43vrxUA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16af73b1-46bb-4a27-d0e2-e7e7a9dea531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<START>', 'Je', 'pense', 'que', 'ça', 'aide.', '<END>'] ['<START>', 'Je', 'pense', 'que', 'ça', 'aide.', '<END>']\n",
            "['<START>', 'Il', 'était', 'trop', 'dur.', '<END>'] ['<START>', 'Il', 'était', 'trop', 'dur.', '<END>']\n",
            "['<START>', 'Écrivez', 'votre', 'nom', 'en', 'majuscules.', '<END>'] ['<START>', 'Écrivez', 'votre', 'nom', 'en', 'majuscules.', '<END>']\n",
            "['<START>', 'Où', 'séjournes-tu', '?', '<END>'] ['<START>', 'Où', 'séjournes-tu', '?', '<END>']\n",
            "['<START>', 'Elle', 'entrouvrit', 'la', 'porte.', '<END>'] ['<START>', 'Elle', 'entrouvrit', 'la', 'porte.', '<END>']\n",
            "['<START>', 'Je', 'ne', 'suis', 'pas', 'intimidée.', '<END>'] ['<START>', 'Je', 'ne', 'suis', 'pas', 'intimidée.', '<END>']\n",
            "['<START>', \"C'est\", 'gentil.', '<END>'] ['<START>', \"C'est\", 'gentil.', '<END>']\n",
            "['<START>', 'Personne', 'ne', 'le', 'saura.', '<END>'] ['<START>', 'Personne', 'ne', 'le', 'saura.', '<END>']\n",
            "['<START>', 'Ne', 'va', 'pas', 'là.', '<END>'] ['<START>', 'Ne', 'va', 'pas', 'là.', '<END>']\n",
            "['<START>', 'La', 'maison', 'est', 'inoccupée.', '<END>'] ['<START>', 'La', 'maison', 'est', 'inoccupée.', '<END>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  for idx in range(num_samples, num_samples+5):\n",
        "    a = model(src_tokens[idx])\n",
        "    predicted_itos = [tgt_vocab.get_itos()[x.item()] for x in a] \n",
        "    ground_truth   = [tgt_vocab.get_itos()[x.item()] for x in tgt_tokens[idx]] \n",
        "    print(predicted_itos,ground_truth)"
      ],
      "metadata": {
        "id": "RBwvVDSExQ11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc9554a5-0141-410b-f3c7-a56868e4a0a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<START>', 'Il', 'était', 'trop', 'dur.', '<END>'] ['<START>', 'Nous', 'avons', 'toutes', 'nos', 'secrets.', '<END>']\n",
            "['<START>', 'Je', 'pense', 'que', 'ça', 'aide.', '<END>'] ['<START>', \"J'aime\", 'cette', 'chambre.', '<END>']\n",
            "['<START>', 'La', 'maison', 'est', 'inoccupée.', '<END>'] ['<START>', 'Nous', 'ne', 'pouvons', 'pas', \"l'aider.\", '<END>']\n",
            "['<START>', 'Où', 'séjournes-tu', '?', '<END>'] ['<START>', 'Ils', 'sont', 'nos', 'invités.', '<END>']\n",
            "['<START>', 'Où', 'séjournes-tu', '?', '<END>'] ['<START>', 'Voulez-vous', 'coucher', 'avec', 'moi', '?', '<END>']\n"
          ]
        }
      ]
    }
  ]
}